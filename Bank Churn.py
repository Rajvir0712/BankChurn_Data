# -*- coding: utf-8 -*-
"""Bank_Churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AN8qnQ4o2OI4Ghb0wQ5PMhgTil3NDWlT

###Importing Libraries
"""

#!pip install -U autogluon > /dev/null

#RT
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency

"""###Data Preprocessing and EDA"""

#RT
data_train_url = "https://raw.githubusercontent.com/Rajvir0712/BankChurn_Data/main/train-3.csv"
data_test_url = "https://raw.githubusercontent.com/Rajvir0712/BankChurn_Data/main/test-2.csv"
data_churn_url = "https://raw.githubusercontent.com/Rajvir0712/BankChurn_Data/main/Churn_Modelling.csv"
df_train = pd.read_csv(data_train_url)
df_test = pd.read_csv(data_test_url)
df_churn = pd.read_csv(data_churn_url)

#RT
df_churn.rename(columns = {'RowNumber':'id'}, inplace = True)

#RT
df_train = pd.concat([df_train, df_churn], ignore_index=True)



#RT
df_train.drop_duplicates()

df_train.head()

#RT
from sklearn.base import BaseEstimator, TransformerMixin
class TotalProductsTransformer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_copy = X.copy()
        X_copy['Total_Products_Used'] = X_copy['NumOfProducts'] + X_copy['HasCrCard']
        return X_copy

TotalProductsTransformer().fit_transform(df_train)
TotalProductsTransformer().fit_transform(df_test)

#RT
df_train['TenureByAge'] = df_train.Tenure/(df_train.Age)
df_test['TenureByAge'] = df_test.Tenure/(df_test.Age)

#RT
df_train['Products_Per_Tenure'] =  df_train['Tenure'] / df_train['NumOfProducts']
df_test['Products_Per_Tenure'] =  df_test['Tenure'] / df_test['NumOfProducts']

#RT
df_train['IsActive_by_CreditCard'] = df_train['HasCrCard'] * df_train['IsActiveMember']
df_test['IsActive_by_CreditCard'] = df_test['HasCrCard'] * df_test['IsActiveMember']

#RT
df_train['QualityOfBalance'] = pd.cut(df_train['Balance'], bins=[-1,100,1000,10000,50000,1000000], labels=['VeryLow', 'Low', 'Medium','High','Highest'])
df_train['QualityOfBalance'].replace(['VeryLow', 'Low', 'Medium','High','Highest'],[0,1,2,3,4], inplace=True)

df_test['QualityOfBalance'] = pd.cut(df_test['Balance'], bins=[-1,100,1000,10000,50000,1000000], labels=['VeryLow', 'Low', 'Medium','High','Highest'])
df_test['QualityOfBalance'].replace(['VeryLow', 'Low', 'Medium','High','Highest'],[0,1,2,3,4], inplace=True)

#RT
df_train['Customer_Status'] = df_train['Tenure'].apply(lambda x:0 if x < 2 else 1)
df_test['Customer_Status'] = df_test['Tenure'].apply(lambda x:0 if x < 2 else 1)
df_train['Balance_to_Salary_Ratio'] = df_train['Balance'] / df_train['EstimatedSalary']
df_test['Balance_to_Salary_Ratio'] = df_test['Balance'] / df_test['EstimatedSalary']



#Checked and no null values
print(df_train['Exited'].value_counts(normalize=True))

print(df_train['Geography'].value_counts(normalize=True))

print(df_train['NumOfProducts'].value_counts(normalize=True))

#RT

# Create the new feature in the training data
df_train['Geo_Gender'] = df_train['Geography'] + '_' + df_train['Gender']

# Create the new feature in the testing data
df_test['Geo_Gender'] = df_test['Geography'] + '_' + df_test['Gender']

# Now, both df_train and df_test have the new 'Geo_Gender' feature
# You might want to check the first few rows to see the new feature
#print(df_train.head())

#RT
# Convert numeric columns to strings and then concatenate
df_train['AGE_SCORE'] = df_train['Age'].astype(str) + '_' + df_train['CreditScore'].astype(str)
df_test['AGE_SCORE'] = df_test['Age'].astype(str) + '_' + df_test['CreditScore'].astype(str)

# Now, both df_train and df_test have the new 'AGE_SCORE' feature
print(df_train.head())

#RT
# One-hot encoding the 'NumOfProducts' column
df_train = pd.get_dummies(df_train, columns=['NumOfProducts'])
df_test = pd.get_dummies(df_test, columns=['NumOfProducts'])

#RT
# One-hot encoding the 'Geography' column
df_train = pd.get_dummies(df_train, columns=['Geography'])
df_test = pd.get_dummies(df_test, columns=['Geography'])



#RT
# One-hot encoding the 'Geography' column
df_train = pd.get_dummies(df_train, columns=['Gender'])
df_test = pd.get_dummies(df_test, columns=['Gender'])

df_train.head()

df_test.head()



mean_tenure = df_train['Tenure'].mean()

print("Mean Tenure:", mean_tenure)

mean_tenure = df_train['Tenure'].median()

print("Median Tenure:", mean_tenure)

# Assuming 'df' is your DataFrame
plt.hist(df_train['Tenure'], bins='auto')  # 'auto' selects an optimal number of bins
plt.xlabel('Tenure')
plt.ylabel('Frequency')
plt.title('Histogram of Tenure')
plt.show()

#RT
bins = [0, 3.32, 6.67, df_train['Tenure'].max() + 0.01]
labels = ['Low Tenure', 'Medium Tenure', 'High Tenure']
df_train['TenureGroup'] = pd.cut(df_train['Tenure'], bins=bins, labels=labels, include_lowest=True)
print(df_train[['Tenure', 'TenureGroup']])

#RT
bins = [0, 3.32, 6.67, df_train['Tenure'].max() + 0.01]
labels = ['Low Tenure', 'Medium Tenure', 'High Tenure']
df_test['TenureGroup'] = pd.cut(df_test['Tenure'], bins=bins, labels=labels, include_lowest=True)
print(df_test[['Tenure', 'TenureGroup']])

df_train.head()

contingency_table = pd.crosstab(df_train['TenureGroup'], df_train['Exited'])

# Perform the Chi-Squared test
chi2, p, dof, expected = chi2_contingency(contingency_table)
print(f"Chi-Squared: {chi2}")
print(f"P-value: {p}")
print(f"Degrees of Freedom: {dof}")
print("Expected frequencies:")
print(expected)

#RT
def age_tr(df) :
    df['Age_Category'] = pd.cut(df['Age'], bins=[18, 30, 40, 50, 60, 100], labels=['18-30', '30-40', '40-50', '50-60', '60+'],include_lowest = True)
    return df

df_train = age_tr(df_train)
df_test = age_tr(df_test)

df_train.head()

# One-hot encoding the 'NumOfProducts' column
df_train = pd.get_dummies(df_train, columns=['Age_Category'])
df_test = pd.get_dummies(df_test, columns=['Age_Category'])

#RT
# One-hot encoding the 'NumOfProducts' column
df_train = pd.get_dummies(df_train, columns=['TenureGroup'])
df_test = pd.get_dummies(df_test, columns=['TenureGroup'])

# Assuming the range of EstimatedSalary is from 0 to 200000 as per the histogram
bin_edges = [0, 20000, 40000, 60000, 80000, 100000, 120000, 140000, 160000, 180000, 200000]
bin_labels = ['0-20k', '20-40k', '40-60k', '60-80k', '80-100k', '100-120k', '120-140k', '140-160k', '160-180k', '180-200k']

# Applying binning to the DataFrame
df_train['SalaryBin_Custom'] = pd.cut(df_train['EstimatedSalary'], bins=bin_edges, labels=bin_labels)


# Now the DataFrame 'df' has additional columns with the binned data

# Assuming the range of EstimatedSalary is from 0 to 200000 as per the histogram
bin_edges = [0, 20000, 40000, 60000, 80000, 100000, 120000, 140000, 160000, 180000, 200000]
bin_labels = ['0-20k', '20-40k', '40-60k', '60-80k', '80-100k', '100-120k', '120-140k', '140-160k', '160-180k', '180-200k']

# Applying binning to the DataFrame
df_test['SalaryBin_Custom'] = pd.cut(df_test['EstimatedSalary'], bins=bin_edges, labels=bin_labels)

# One-hot encode the 'SalaryBin_Custom' column if it has not been done yet
df_train = pd.get_dummies(df_train, columns=['SalaryBin_Custom'])
# One-hot encode the 'SalaryBin_Custom' column if it has not been done yet
df_test = pd.get_dummies(df_test, columns=['SalaryBin_Custom'])

#RT
import pandas as pd

def cred_score_tr(df):
    bins = list(range(350, 851, 50))
    labels = [f'{i}-{i+49}' for i in bins[:-1]]

    df['Credit_Score_Range'] = pd.cut(df['CreditScore'], bins=bins, labels=labels, include_lowest=True)
    df['Credit_Score_Range'] = df['Credit_Score_Range'].astype('object')
    return df

# Apply the function to both training and testing data
df_train = cred_score_tr(df_train)
df_test = cred_score_tr(df_test)

df_train.info()

# Import necessary libraries
import xgboost as xgb
from xgboost import XGBClassifier
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# Encode categorical columns
label_encoders = {}
for column in ['Surname', 'QualityOfBalance', 'Geo_Gender', 'AGE_SCORE', 'Credit_Score_Range']:
    label_encoders[column] = LabelEncoder()
    X_train[column] = label_encoders[column].fit_transform(X_train[column])

# Your XGBoost parameters
xgb_params = {
    'alpha': 2.1401971831451174e-07,
    'subsample': 0.8154621328436412,
    'colsample_bytree': 0.6993225521103127,
    'max_depth': 6,
    'min_child_weight': 3,
    'learning_rate': 0.09326219307123645,
    'gamma': 0.0035058360119452673,
    'scale_pos_weight': 0.81619438771,
    'enable_categorical': True  # Enable support for categorical features
}

# Train the model
model = XGBClassifier(**xgb_params)
model.fit(X_train, y_train)

# Get feature importances and sort them
feature_importances = model.feature_importances_
sorted_idx = np.argsort(feature_importances)[::-1]

# Create the sorted bar chart
plt.figure(figsize=(10, 8))
plt.barh(X_train.columns[sorted_idx], feature_importances[sorted_idx])
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Sorted Feature Importance Bar Chart')
plt.gca().invert_yaxis()  # To display the highest importance at the top
plt.show()

#RT
# Retrain the model on the entire training dataset

import pandas as pd
import xgboost as xgb
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold
import numpy as np

# Preparing data
X = df_train.drop('Exited', axis=1)  # Features
y = df_train['Exited']               # Target
X_encoded = pd.get_dummies(X)


xgb_params = {'alpha': 2.1401971831451174e-07,
                           'subsample': 0.8154621328436412,
                           'colsample_bytree': 0.6993225521103127,
                           'max_depth': 6,
                           'min_child_weight': 3,
                           'learning_rate': 0.09326219307123645,
                           'gamma': 0.0035058360119452673,
                           'scale_pos_weight': 0.81619438771}

final_model = xgb.XGBClassifier(**xgb_params)
final_model.fit(X_encoded, y)

# Assuming df_test is your test DataFrame and it has been preprocessed similarly to df_train
df_test_encoded = pd.get_dummies(df_test)
df_test_encoded = df_test_encoded.reindex(columns=X_encoded.columns, fill_value=0)

# Make predictions on the test dataset
test_pred_proba = final_model.predict_proba(df_test_encoded)[:, 1]

# Create a submission DataFrame
# Ensure 'id' matches the name of the identifier column in your test dataset
submission_df = pd.DataFrame({
    'id': df_test['id'],
    'Exited': test_pred_proba
})

# Save the submission file
submission_filename = 'xgboost_submission_1.csv'
submission_df.to_csv(submission_filename, index=False)

print(f"Submission file '{submission_filename}' created successfully.")

df_train.info()

from autogluon.tabular import TabularDataset, TabularPredictor

train = TabularDataset(df_train)
test = TabularDataset(df_test)

automl = TabularPredictor(label='Exited', problem_type='binary', eval_metric='roc_auc')
automl.fit(train, presets='best_quality')

automl.leaderboard()

from autogluon.tabular import TabularDataset, TabularPredictor

# Load your trained AutoML model
# Make sure the path is where your AutoGluon model is saved
predictor = TabularPredictor.load("AutogluonModels/ag-20240114_105858")

# Identify the best model from the leaderboard
best_model = predictor.get_model_best()

# Load your test data
# Assuming df_test is your test DataFrame and it contains an 'id' column
df_test_encoded = pd.get_dummies(df_test)
df_test_encoded = df_test_encoded.reindex(columns = predictor.feature_metadata_in.get_features(), fill_value=0)

# Make predictions using the best model
# If your task requires probabilities, use `predict_proba`
predictions = predictor.predict_proba(df_test_encoded, model=best_model)[:, 1]

# Create a DataFrame for submission
submission_df = pd.DataFrame({
    'id': df_test['id'],  # Replace 'id' with the actual ID column name from your test dataset
    'Exited': predictions  # This column should be named as per the submission requirements
})

# Save the submission file
submission_df.to_csv('autogluon_submission.csv', index=False)

# Display the first few rows of the submission file
print(submission_df.head())

df_train.head()

from autogluon.tabular import TabularDataset, TabularPredictor

# Load your trained AutoML model
predictor = TabularPredictor.load("AutogluonModels/ag-20240114_124036")

# Identify the best model from the leaderboard
best_model = predictor.get_model_best()

# Load your test data
df_test_encoded = pd.get_dummies(df_test)
df_test_encoded = df_test_encoded.reindex(columns = predictor.feature_metadata_in.get_features(), fill_value=0)

# Make predictions using the best model
predictions_proba = predictor.predict_proba(df_test_encoded, model=best_model)

# Extract the probabilities for the positive class
# Assuming binary classification, the positive class is usually at index 1
positive_class_probabilities = predictions_proba.iloc[:, 1]

# Create a DataFrame for submission
submission_df = pd.DataFrame({
    'id': df_test['id'],  # Make sure 'id' matches the ID column name in your test dataset
    'Exited': positive_class_probabilities
})

# Save the submission file
submission_df.to_csv('autogluon_submission.csv', index=False)

# Display the first few rows of the submission file
print(submission_df.head())